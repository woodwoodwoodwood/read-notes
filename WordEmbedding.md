# Word Embedding

## 文本表示（Representation）:

文本是一种非结构化的数据信息，是不可以直接被计算的。

**文本表示的作用就是将这些非结构化的信息转化为结构化的信息**，这样就可以针对文本信息做计算，来完成我们日常所能见到的文本分类，情感判断等任务。

![image-20240924003533150](./assets/image-20240924003533150.png)

文本表示的方法有很多种：

1. 独热编码：无法表达词语之间的关系；这种过于稀疏的向量，导致计算和存储的效率都不高

2. word-embedding：可以将文本通过一个低维向量来表达，不像 one-hot 那么长。语意相似的词在向量空间上也会比较相近。

![image-20240924004501505](./assets/image-20240924004501505.png)

## 2 种主流的 word embedding 算法

### (1) word2vec

 word2vec 算法通过对大型语料库中的文本进行建模来估计这些表示。经过训练，这样的模型可以检测同义词或为部分句子建议附加单词。 Word2vec 由Tomáš Mikolov和Google的同事开发，于 2013 年发布。

Word2vec 将单词表示为高维数字向量，用于捕获单词之间的关系。特别是，出现在相似上下文中的单词被映射到通过**余弦相似度**(consing similarity)测量的附近的向量。这表明单词之间的**语义相似程度**(semantic similarity)，例如， “walk”和”ran”的向量很接近，“but”和“however”以及“Berlin”和“Germany”的向量也很接近。

Word2vec 将大型[文本语料库](https://en.wikipedia.org/wiki/Text_corpus)作为输入，并生成一个[向量空间](https://en.wikipedia.org/wiki/Vector_space)，通常有数百[维](https://en.wikipedia.org/wiki/Dimensions)，[语料库](https://en.wikipedia.org/wiki/Corpus_linguistics)中的每个唯一单词都在该空间中分配一个相应的向量。

#### 1）跳元模型（Skip-Gram）

![image-20240924005800758](./assets/image-20240924005800758.png)

#### 2）连续词袋（CBOW）模型

![image-20240924005823912](./assets/image-20240924005823912.png)

***

### (2) Glove





***

## Bert中的WordPiece

**WordPiece** 是一种**子词级别的分词算法**，最早由 Google 在其机器翻译模型中提出，后来被用于 BERT 等模型中，用来解决自然语言处理中的词汇稀疏性问题。与传统的按词分割（word-level tokenization）不同，WordPiece 将词分解为更小的子词单元，能够更有效地处理低频词、稀有词以及新词汇。

**WordPiece** 是 Google 开发的用于预训练 BERT 的标记化算法。此后，它已在相当多基于 BERT 的 Transformer 模型中得到重用，例如 DistilBERT、MobileBERT、Funnel Transformers 和 MPNET。它在训练方面与 BPE 非常相似，但实际的标记化却有所不同。

### 1. 为什么需要 WordPiece？

在自然语言处理任务中，词汇表通常很大，而有些词非常罕见甚至从未出现在训练数据中，这会导致所谓的“词汇外”（Out-Of-Vocabulary，OOV）问题。为了解决这个问题，WordPiece 通过将罕见词分割成子词单元，使模型即使在遇到未见过的词时也能通过已见过的子词来理解它。

例如，WordPiece 会将“**unhappiness**”这样的词分解为更常见的子词“**un**”、“**happiness**”，其中“**happiness**”还可以进一步分解为“**happy**”和“**ness**”。

### 2. WordPiece原理

WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。

BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。

比如"loved","loving","loves"这三个单词。其实本身的语义都是“爱”的意思，但是如果我们以单词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。

BPE算法通过训练，能够把上面的3个单词拆分成"lov","ed","ing","es"几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。

WordPiece 的分词过程是基于词频和贪婪算法的。以下是其核心工作原理：

- **初始化词汇表**：首先，词汇表只包含所有的单个字符，即词汇表中的基础单位是字母或字符。

- **合并高频子词对**：在初始词汇表的基础上，算法逐步合并在文本中高频出现的子词对（字符对或已有子词对）。具体来说，每次合并最频繁的子词对，将其添加到词汇表中，直到词汇表的大小达到预定限制。

  例如，如果“**un**”和“**happy**”在文本中频繁出现，WordPiece 可能会首先将它们合并为“**unhappy**”，再进一步处理其他高频子词。

- **贪婪选择**：当模型遇到一个词时，会尽可能选择最长的匹配子词进行分割。例如，模型会将“**unhappiness**”分割为“**un** + **happiness**”，而不是单独的字母或更短的子词。

### 3. **WordPiece 分词示例**：

假设有以下句子：

```
The dogs are running happily.
```

如果词汇表中没有“running”和“happily”这两个词，WordPiece 可能会将它们分解为更小的子词：

- **running** → “run” + “##ning” （“##”表示这是一个子词片段）
- **happily** → “happy” + “##ly”

这种分词方式确保了子词之间的关系保持完整，即使是模型未见过的新词或低频词，也可以通过子词拼接来理解。

### 4. **与BPE算法的对比**：

**WordPiece** 和 **BPE（Byte Pair Encoding）** 都是子词级别的分词算法，它们的目标都是将词分解为更小的子词单元来表示，以解决词汇稀疏性和词汇外（OOV）问题。

**BPE（Byte Pair Encoding）**：

- BPE 是基于**字符对的频率**来构建词汇表的算法。
- 它从最初的单个字符开始，每次找到频率最高的字符对，并将这些字符对合并为一个新的符号，然后继续合并直到词汇表达到预定大小。
- BPE 的基本思想是贪婪算法，它通过最大化高频字符对的合并，逐渐生成子词单元，避免直接处理每个完整的单词。

**WordPiece**：

- WordPiece 也是基于子词单元的分词方法，但它的核心区别在于它使用的是基于**概率最大化**而不是字符对频率的策略。
- WordPiece 在生成子词单元时，不仅考虑频率，还考虑通过合并某个子词对提升语言模型训练的对数似然值。每次它选择最能提升语言模型的子词对进行合并，而不是简单地基于频率。

总结：

- **BPE** 更注重频次统计，合并策略简单高效，但对语义一致性考虑不足，可能导致次优分词。
- **WordPiece** 则通过最大化概率和语言模型训练效果进行分词，词汇表质量更高，特别适合需要上下文感知的复杂自然语言处理任务。



### 5. **WordPiece 的优点**：

- **减少 OOV 问题**：通过子词单元的分割，WordPiece 极大减少了未见过的词汇问题，因为即使是罕见词也可以分解为常见的子词。
- **高效处理低频词**：WordPiece 能够将低频词和新词合理分割成已知的子词，因此在实际应用中表现出色。
# GPT: Improving Language Understanding by Generative Pre-Training

介绍了一种通过生成式预训练（GPT）提高语言理解的半监督学习方法。其核心思想是通过大量无监督数据的预训练，结合监督学习的微调，使得模型可以有效转移到各种自然语言理解任务上。

## 1. **背景与问题**

自然语言理解任务（如文本蕴含、问答、语义相似度评估和文档分类）通常需要大量的标注数据，而标注数据的获取成本高且在很多领域较为稀缺。大多数深度学习模型依赖大量的标注数据，限制了这些模型在许多领域的应用。然而，无监督数据相对充足，因此如何利用无监督的语言信息来改善模型的表现，成为解决这个问题的关键。

从**未标注文本中提取超过词级别的信息**所面临的两个主要挑战

1. **优化目标的不确定性**：要从无标注数据中学习对下游任务有用的文本表示，首先需要定义一个合适的优化目标。然而，目前还不清楚哪种优化目标最适合这种学习目的。
2. **如何将学习到的表示迁移到目标任务**：即便通过无监督学习获得了良好的文本表示，如何将这些表示有效地应用于具体的有监督任务仍然存在问题。

不同的优化目标在不同任务上表现不同，因此**很难确定哪种优化目标能最好地学习通用的文本表示**；其次，即便学到了良好的文本表示，**将这些表示有效地迁移到特定任务中也充满挑战**。当前研究在如何构建统一的半监督学习方法上还没有达成一致。



## 2. **方法概述**

论文提出了一种两阶段的半监督学习方法：

1. **无监督预训练**：首先使用大规模无标注的文本数据训练语言模型，通过语言建模任务学习初始的模型参数。
2. **有监督微调**：然后将这些预训练得到的参数，应用于具体的有监督任务，通过少量标注数据进行微调。

这种方法能够让模型在转移到不同任务时进行有效的知识迁移，并且只需要对模型架构进行最小的修改。

首先，模型在大规模未标注文本数据上进行无监督的语言模型预训练，目标是通过预测下一个单词来学习上下文表示。公式为：

$ L(U) = \sum_{i} \log P(u_i | u_{i-k}, ..., u_{i-1}; \Theta) $

其中，$k$ 是上下文窗口的大小，$P$ 是模型用于条件概率预测的神经网络，$\Theta$ 是模型的参数。模型结构采用了基于Transformer的解码器架构，通过自注意力机制学习输入token的上下文信息。

![image-20240920161356154](C:\Users\JiangLei\AppData\Roaming\Typora\typora-user-images\image-20240920161356154.png)





## 3. **模型架构**

该模型使用了Transformer架构，这是因为Transformer在处理长距离依赖关系上表现良好，并且在多个任务上显示出强大的性能。具体而言，作者使用了一个多层的Transformer解码器进行语言建模，并在无监督预训练阶段通过自注意力机制和位置嵌入学习语言上下文表示。

在微调阶段，模型通过对任务特定输入的转换来适应不同的任务，并通过最小化任务的监督目标对预训练的模型参数进行调整。

### (1) **输入和任务定义**

首先，作者定义了有监督任务的输入和目标：

- 假设有一个**标注数据集 C**，其中每个样本由输入序列 $ x^1, x^2, ..., x^m$ 和对应的标签 $y$ 组成。
- 输入的序列通过已经预训练好的模型，生成最终的Transformer激活状态 $h_l^m $，其中 $l$ 是层数，$m$ 是输入序列的长度。

### (2) **线性层和输出概率**

预训练的模型生成激活状态后，输出会被输入到一个**额外的线性层**，通过该层模型可以进行分类或预测：

- 公式 $P(y∣x1,...,xm)=softmax(h_l^mW_y)$ 描述了预测过程：
  - $h_l^m$ 是最后一层 Transformer 的激活状态，代表输入序列的上下文表示。
  - $W_y$ 是在线性层中用于映射激活状态到标签的参数矩阵。
  - 通过 **softmax** 函数，模型将这些激活值转换为每个可能类别$y$ 的概率分布。

### (3) **监督目标函数**

为了微调模型，使其适应特定的有监督任务，作者定义了一个监督目标函数：

- **公式 $L_2(C) = \sum_{(x,y)} \log P(y | x^1, ..., x^m)$** 

  表示对所有训练样本进行加总，最大化其对应的概率。这是一个典型的交叉熵损失函数，常用于分类任务，目标是最大化正确标签 $y$ 的概率。

### (4)**语言模型作为辅助任务**

为了进一步提升模型的微调效果，作者引入了**语言建模任务**作为辅助目标：

- 他们发现，将语言建模加入到微调过程中，可以通过以下方式提高模型性能：
  - **改善泛化能力**：通过保持语言建模的目标，模型能够在有限的标注数据上更好地适应新任务，减少过拟合。
  - **加速收敛**：由于模型在无监督预训练时已经学习了良好的语言表示，继续保持这个目标有助于快速微调。

### (5) **辅助目标函数**

引入语言建模作为辅助目标后，整体的优化目标变为：

- **公式 $L_3(C) = L_2(C) + \lambda \cdot L_1(C)$**：

  这里，$L_2(C)$ 是原来的监督目标，$L_1(C)$ 是无监督的语言模型损失，$\lambda$ 是调节两个目标权重的超参数。这种组合有助于同时保持语言模型和分类任务的目标，确保模型在微调过程中获得更好的效果。

### (6) **额外参数**

微调过程中，唯一需要调整的额外参数是**线性层参数 $W_y$**，以及用于表示输入序列分隔符的词嵌入。这意味着除了最后一层的线性变换外，模型的大部分参数都保留了预训练时学到的知识。

1. **$L_1(C)$**：语言模型损失（Unsupervised Language Modeling Loss）

   - **定义**：$L_1(C)$ 是无监督的**语言模型损失**，对应的是在无监督预训练阶段的目标。

   - **作用**：这个损失函数用于语言建模任务，即模型根据上下文预测下一个单词或token。通过最大化语言模型的概率，模型能够学习文本的语法和语义结构，以及单词之间的关系。

2. **$L_2(C)$**：监督任务损失（Supervised Task Loss）

   - **定义**：$L_2(C)$ 是**监督任务的损失函数**，在微调阶段使用。这是模型在处理特定任务（例如分类、问答或文本蕴含）时，最主要的优化目标。

   - **作用**：这个损失函数通过最大化正确类别的概率，直接将模型训练为能够完成具体的任务。例如，在分类任务中，$L_2(C)$ 可能是交叉熵损失函数，它用于最大化正确类别的预测概率。

3. **$L_3(C)$**：联合损失（Combined Loss）

   $L_3(C) = L_2(C) + \lambda \cdot L_1(C)$

   - **$L_3(C)$** 是微调过程中要优化的最终损失函数。它是通过将**语言模型损失 $L_1(C)$** 和**监督任务损失 $L_2(C)$** 结合起来获得的。
   - $\lambda$ 是权重系数，控制语言模型损失对总损失的影响程度。通过这种联合损失，模型既能学习特定任务的目标，又能保持对语言常识的掌握。

   

## 4. **输入转换**

不同的任务（如文本分类、问答、文本蕴含）在输入结构上有所差异，因此需要特定的输入转换：

- **文本蕴含任务**：将前提和假设拼接成一个输入序列，用特殊的分隔符进行区分。
- **相似度任务**：将待比较的两个句子进行双重排列，每种排列独立处理后再汇总。
- **问答任务**：将文档和问题与每个可能的答案拼接在一起，分别处理后进行归一化。



![GPT-mutiple-tasks](C:\Users\JiangLei\Learn\github\read-notes\assets\GPT-mutiple-tasks.png)

每个任务将输入结构化的文本转换为模型能够处理的token序列，具体如下：

1. **分类任务（Classification）**

   - 输入是**文本**，前面加上了特殊标记（Start），用来表示句子的开始。

   - 经过 Transformer 后，提取出重要特征，最后通过线性层进行分类。

   - **实例**：

     - **输入文本**： "The movie was absolutely wonderful."

     - **任务目标**：将这段文本分类为“积极”（positive）或“消极”（negative）。

     - **输入转换**

       ：添加一个特殊标记表示句子开始，序列如下：

       ```bash
       [Start] The movie was absolutely wonderful.
       ```

     - **输出**：经过Transformer层处理后，模型输出一个向量表示，将其输入线性层，通过softmax得到分类结果，如“积极”。

2. **文本蕴含任务（Entailment）**

   - 输入是两个句子：**前提（Premise）和 假设（Hypothesis）**。

   - 两者通过分隔符（Delim）分开，将它们拼接成一个序列后输入到Transformer中。

   - Transformer 输出句子的上下文表示，经过线性层后进行分类，判断假设是否可以从前提推导出来（文本蕴含）。

   - **实例**：

     - **前提（Premise）**： "A man is playing a guitar."

     - **假设（Hypothesis）**： "A man is making music."

     - **任务目标**：判断假设句子是否可以从前提推导出来，可能的标签为“蕴含”（entailment）、“矛盾”（contradiction）或“中立”（neutral）。

     - **输入转换**

       ：将前提和假设拼接成一个序列，使用分隔符（Delim）分开，序列如下：

       ```
       [Start] A man is playing a guitar. [Delim] A man is making music.
       ```

     - **输出**：经过Transformer层处理后，模型输出一个向量表示，经过线性层后预测标签，如“蕴含”。

3. **相似度任务（Similarity）**

   - 输入是两个句子：**Text 1** 和 **Text 2**。

   - 这两个句子通过分隔符（Delim）分隔，然后将它们分别输入到Transformer中（图中有两个独立的Transformer块）。

   - 通过两个Transformer分别处理这两个句子后，将它们的输出表示通过加和（或其他方式）融合在一起，最后通过线性层判断它们的相似度。

   - **实例**：

     - **句子1（Text 1）**： "The cat is sitting on the mat."

     - **句子2（Text 2）**： "A feline rests on the rug."

     - **任务目标**：判断这两个句子是否表达了相似的含义，通常评分范围为0到1。

     - 输入转换

       ：将两个句子通过分隔符分开后输入到各自的Transformer块，序列如下：

       ```
       [Start] The cat is sitting on the mat. [Delim] A feline rests on the rug.
       ```

     - **输出**：分别通过两个Transformer块处理，然后将输出进行融合（如向量加和或拼接），经过线性层，得到相似度评分。

4. **多项选择任务（Multiple Choice）**

   - 输入是上下文**Context**和多个选项（**Answer 1, Answer 2, ..., Answer N**）。

   - 每个答案与上下文组合，通过分隔符分开。每个组合分别输入到Transformer中处理。

   - 最终每个组合的输出会通过线性层，然后通过softmax选择最可能的答案。

   - **实例**：

     - **上下文（Context）**： "The capital city of France is ____."

     - **选项（Answers）**：

       1. Paris
       2. London
       3. Rome
       4. Berlin

     - **任务目标**：从选项中选择正确的答案（Paris）。

     - **输入转换**

       ：将上下文和每个选项分别拼接起来，通过分隔符分开，形成多个输入序列：

       ```
       [Start] The capital city of France is ____. [Delim] Paris
       [Start] The capital city of France is ____. [Delim] London
       [Start] The capital city of France is ____. [Delim] Rome
       [Start] The capital city of France is ____. [Delim] Berlin
       ```

     - **输出**：每个拼接后的输入序列都经过Transformer层处理，然后分别通过线性层，最后通过softmax选择得分最高的答案，即Paris。

     

## 5. 模型规格

**Transformer架构**：这个模型的架构是基于原始Transformer的工作，但这里的模型是一个**仅使用解码器的12层Transformer**。该架构中的每一层都采用了**掩码（masked）自注意力机制**。

- **维度设置：**
  - 每个Transformer层有**768维的状态表示**。
  - 每一层使用**12个注意力头**，每个头用于捕捉不同的注意力模式。
- **前馈网络**：每层Transformer后接一个**位置前馈网络**（Position-wise Feed-forward Network），该网络的隐藏层为**3072维**。

### Transformer解码器块的组成：

1. **自注意力机制（Self-Attention Mechanism）**
   - 包含三个部分：**Q（查询）**、**K（键）** 和 **V（值）** 矩阵，以及输出投影矩阵。
2. **前馈网络（Feed-Forward Network）**
   - 包含两个线性层，分别将768维映射到3072维，再映射回768维。
3. **LayerNorm（层归一化）**
   - 在自注意力和前馈网络之后都有LayerNorm。

### 1. **自注意力机制（Self-Attention Mechanism）**

对于每个自注意力机制，计算**Q**、**K** 和 **V** 以及输出投影矩阵的参数量：

- **Q、K、V矩阵**：每个矩阵的形状是 [768, 768]，共3个矩阵（Q、K、V）。
  - 参数量 = $768 \times 768 \times 3 = 1,769,472 \approx 1.77M $ 个参数
- **输出投影矩阵**：形状为 `[768, 768]`。
  - 参数量 = $768 \times 768 = 589,824 \approx 5.9M$ 个参数
- **自注意力机制总参数量**：
  - 总参数量 = $1,769,472 + 589,824 = 2,359,296 \approx 2.36M$ 个参数

### 2. **前馈网络（Feed-Forward Network）**

前馈网络包含两个线性层：

- **第一层线性层**：将输入从768维映射到3072维，参数量为 [768, 3072]。
  - 参数量 = $768 \times 3072 = 2,359,296 \approx 2.36M$个参数
- **第二层线性层**：将输入从3072维映射回768维，参数量为 [3072, 768]。
  - 参数量 = $3072 \times 768 = 2,359,296 \approx 2.36M$ 个参数
- **前馈网络总参数量**：
  - 总参数量 = $2,359,296+2,359,296=4,718,592 \approx 4.72M$个参数

### 3. **LayerNorm（层归一化）**

每个Transformer块包含两个LayerNorm层（一个在自注意力后，一个在前馈网络后）。每个LayerNorm的参数量为2倍的输入维度（因为需要均值和方差），每个LayerNorm有：

- LayerNorm参数量：
  - 每个LayerNorm的参数量 = $768 \times 2 = 1,536$ 个参数
  - 总共有2个LayerNorm层，因此每个块中的LayerNorm参数量 = $1,536 \times 2 = 3,072$ 个参数

### 4. **一个Transformer解码器块的总参数量**

将每个部分的参数量相加：

- 自注意力机制：$2,359,296 \approx 2.36M$ 个参数
- 前馈网络：$4,718,592 \approx 4.72M$ 个参数
- LayerNorm：$3,072$ 个参数
- **总参数量（每层）** = $2,359,296+4,718,592+3,072=7,080,960 \approx 7.08M$ 个参数

### 5. **12层解码器块的总参数量**

一个Transformer解码器块有$7,080,960$个参数，$12$层的总参数量为：

- **总参数量（12层）** = $7,080,960 \times 12 = 84,971,520 \approx 84.97M$ 个参数

### 6. **词嵌入层和输出线性层的参数**

除了12个解码器块，模型还包括词嵌入层和输出线性层：

- **词嵌入层**：$40,000$个词汇，每个词嵌入$768$维，总参数量为：
  - $40,000 \times 768 = 30,720,000 \approx 30.72M$ 个参数
- **输出线性层**：假设输出类别数为1000，则输出线性层参数量为：
  - $768 \times 1000 = 768,000 \approx 0.768M$ 个参数

### 7. **模型的总参数量**

将词嵌入层、12个解码器块和输出线性层的参数量相加：

- 12个解码器块：**84,971,520** 个参数
- 词嵌入层：**30,720,000** 个参数
- 输出线性层：**768,000** 个参数
- **总参数量** = $84,971,520 + 30,720,000 + 768,000 = 116,459,520 \approx 116M$ 个参数

### **总结**

通过先计算单个Transformer解码器块的参数量，然后乘以12层，最后加上词嵌入层和输出线性层的参数，得出该12层Transformer模型的总参数量为约 **116.46M（1.16亿）** 个参数。



## 6. 不同数量的预训练层的影响

![different-layer-effect](C:\Users\JiangLei\Learn\github\read-notes\assets\different-layer-effect.png)

随着从无监督预训练转移到有监督任务的层数增加，模型的表现持续提升。



***

# GPT2：Language Models are Unsupervised Multitask Learners

**无监督学习**（Unsupervised Learning）

**多任务学习**（Multitask Learning）

**零样本学习**（Zero-shot Learning）

## 1. 背景与问题

当前机器学习系统只在特定的数据集上表现不错，**缺乏泛化能力**，并且需要大量的标注数据。目前的机器学习系统更多地表现为狭窄的专家而非能够广泛应对多种任务的通用型系统，**单域数据集**上**单任务训练**是当前机器学习系统缺乏泛化性的主要原因。

为了克服这些局限，文章强调了需要开发能够在多种域和任务上训练和测试性能的鲁棒系统，提出了多任务学习。文中提及的**多任务学习**和**元学习**观点表明，为了实现更好的泛化能力，可能需要大量的有效训练对（数据集和目标注）。

![GPT2-zero-shot](C:\Users\JiangLei\Learn\github\read-notes\assets\GPT2-zero-shot.png)



## 2. 方法概述

### (1) 语言建模

**语言建模**被定义为从一系列示例中无监督地估计分布，这些示例由可变长度的符号序列组成。

$p(x)=∏_{i=1}^np(si∣s1,...,si−1)$

这个公式表示的是在**语言模型**中，如何通过条件概率来计算一个句子或序列的整体概率 $p(x)$。

- $p(x)$ 表示整个序列 $x$ 的联合概率。
- $s_1, s_2, ..., s_n$ 是组成这个序列的符号（如单词或字符）。
- 该公式将序列的联合概率分解为各个符号的条件概率的乘积。也就是说，当前符号 $s_n$ 的概率取决于它之前的所有符号 $s_1, ..., s_{n-1}$。

### (2) 任务条件化

在多任务学习和元学习框架下，如何将执行单一任务扩展为能够处理多个任务的系统？

**单任务学习的概率框架**：

- 在处理单一任务时，学习可以表示为估计一个**条件分布**，即 $p(\text{output}|\text{input})$，表示在给定输入的情况下输出的概率。
- 这种条件分布框架通常用于描述如何从输入推导出正确的输出，适用于一个特定任务。

**多任务学习的扩展**：

- 对于通用系统，应该能够处理**多个不同的任务**，即便是相同的输入。也就是说，系统不应仅基于输入，还应基于**要执行的任务**来做出预测。
- 这意味着，模型应当估计 $p(\text{output}|\text{input}, \text{task})$，不仅考虑输入，还要考虑任务的条件化。

### (3) 无监督学习

语言建模理论上可以学习执行任务，而不需要明确的监督（即不需要指明哪些符号是要预测的输出）。**监督学习的目标**和**无监督学习的目标**本质上是相同的，区别只是监督学习是在序列的一部分进行评估，而无监督学习是在整个序列上。因此，无监督目标的全局最小值也是监督目标的全局最小值。

作者推测，如果语言模型具有**足够的容量**，它将开始**推理和执行任务**，这些任务可能是通过自然语言序列展示的，目的是为了更好地预测这些序列。如果模型能够做到这一点，它实际上就是在执行无监督的多任务学习。

#### 1. **语言建模作为基础**：

- 语言建模的目标是预测一个序列中下一个符号（例如单词）的概率，给定之前的上下文（即前面的符号）。通过这种预测，模型学会了如何理解和生成语言。
- 这种语言建模任务是**无监督的**，模型不需要明确的标签（如特定的输出任务），而是通过大量的语言数据自主学习。

#### 2. **自然语言中的任务隐式学习**：

- 自然语言中包含了大量的任务和信息。例如，翻译任务可以被表示为一段英语文本和其对应的法语翻译；阅读理解任务可以表示为一个文档、问题和答案。语言模型可以通过**观察和学习这些语言序列**，逐渐理解并学会执行这些任务。
- 这意味着，模型不需要显式标记某个任务（如“这是一个翻译任务”或“这是一个问答任务”），而是**通过自然语言的上下文和提示推断出任务是什么**。

#### 3. **无监督目标和监督目标的一致性**：

- 作者提出，无监督的语言建模目标与监督学习的目标是一致的，只是无监督学习是在整个序列上进行优化，而监督学习只是在序列的某个子集上进行。
- 因此，通过优化语言模型来预测整个序列中的符号（即无监督学习），也能在一定程度上**达到监督学习中的任务效果**。例如，通过学习如何生成下一个词，模型也可能学会翻译、回答问题等任务。

#### 4. **模型的推理能力**：

- 作者推测，如果一个语言模型的**容量足够大**，它将能够自动推理出任务，并通过自然语言中的线索来学习如何执行这些任务。这种推理能力允许模型在面对新的任务时，即便没有明确的训练数据，也能够完成任务。

#### 5. **零样本学习的验证**：

- 作者通过**零样本学习**（zero-shot learning）的实验验证了这一推测。零样本学习指的是模型在没有针对某一任务的专门训练下，依然能够完成该任务。作者测试了语言模型在多个不同任务上的表现，结果表明，模型确实具备在没有专门监督数据的情况下完成多任务的能力。

***

## 3. 训练数据集

**传统语言模型的数据来源**：

- 之前的大多数语言模型只在**单一领域的文本**上进行训练，例如新闻文章、维基百科或者小说。
- 作者认为，这种单一领域的训练限制了模型在多种任务上的表现。因此，他们的目标是构建一个尽可能大、尽可能多样化的数据集，涵盖自然语言中的各种任务和上下文。

**数据来源：Common Crawl**：

- Common Crawl 是一个包含大量网络文本的公开爬取数据集，规模比传统语言模型的数据集大许多数量级。尽管如此，Common Crawl 的**数据质量问题**非常严重。之前的工作（如 Trinh & Le 2018 年在常识推理中的研究）使用了这个数据集，但他们注意到大量文档内容难以理解或不具备有效信息。
- 作者在自己的实验中也遇到了类似的问题，很多内容无法准确地表达任务所需的语言信息。

**从网络获取多样性数据**：

- 作者想避免提前对任务做出假设，因此，他们选择不使用像 Trinh & Le 那样特定于某一任务的小样本。
- 为了构建高质量的数据集，他们设计了新的爬取方法，**专注于文档质量**。他们只爬取经过人类筛选/过滤的网页。
- 具体来说，作者选择从 Reddit 这个社交媒体平台上获取链接，爬取那些**获得至少 3 点 Karma**（类似于点赞）的网页。Karma 可以被视为一个启发式指标，表明这些链接被其他用户认为是有趣的、具有教育意义的或有娱乐价值的。

**WebText 数据集的构建**：

- 最终，作者构建了一个名为 **WebText** 的数据集，包含了 Reddit 上 4500 万个链接中的文本内容。
- 为了从网页的 HTML 响应中提取文本，他们使用了 Dragnet 和 Newspaper1 两种内容提取工具。
- 文中所有实验结果都基于 WebText 的一个早期版本，该版本不包括 2017 年 12 月之后创建的链接，经过去重和启发式清理后，数据集中包含了**800 万篇文档**，总计 **40 GB** 的文本。

作者创建了一个多样化且高质量的语言数据集，称为 **WebText**，以用于 GPT-2 的训练。与之前的工作不同，作者选择从 Reddit 获取经过人类筛选的高质量链接，并剔除了低质量内容和常见的维基百科文档，确保数据的多样性和高质量。

## 4. 输入表示

作者改进了**Byte Pair Encoding (BPE)** 技术，以提高语言模型的效率和泛化能力。传统的 BPE 技术在处理字符级和词级语言建模时存在一些问题，特别是在如何合并和生成子词单元（tokens）方面。作者通过修改 BPE 的合并规则，优化了子词表示，避免了低效的符号分配。

### 1.**字符级输入**

是指将文本拆分为单个字符（包括字母、标点、空格等），然后将这些字符作为模型的输入。这种方法不会预先进行任何分词操作，而是将整个文本视作一系列的字符来处理。示例：对于句子 "**dog is cute**"，字符级输入会将其拆分为：

​	`['d', 'o', 'g', ' ', 'i', 's', ' ', 'c', 'u', 't', 'e']`

​	每个字符单独作为模型的一个输入单元。

**优点**：

- **通用性强**：可以处理任何语言或字符集，无需词汇表。
- **不受词表限制**：不存在"词汇外"（out-of-vocabulary, OOV）问题，因为所有字符都是基础单位。

**缺点**：

- **效率低**：每个字符单独作为输入，无法直接利用词级别的上下文信息，序列长度会很长，训练时间会变长。
- **难以捕捉高层次语义**：单个字符缺乏语义信息，难以直接捕捉到词语或句子的意义。

### 2. **词级输入**

将文本按词分割，每个词作为模型的一个输入单元。这种方法依赖于一个**预定义的词汇表**，只有词汇表中出现的词才能作为输入。如果遇到词汇表中没有的词，通常会用特殊标记（如 `[UNK]`）表示。

**示例：**

​	对于句子 "**dog is cute**"，词级输入会是：

​	`['dog', 'is', 'cute']`

​	每个词单独作为输入单元。

**优点**：

- **语义信息丰富**：每个输入单元是完整的词，模型可以直接处理词级的语义信息，容易捕捉上下文语义。
- **高效**：序列较短，模型可以更高效地处理输入。

**缺点**：

- **词汇外问题**：如果一个词不在词汇表中，会产生OOV问题，导致处理新词或罕见词的能力较差。
- **词汇表固定**：需要预先定义一个固定的词汇表，词汇表的大小会影响模型的表现。

### 3. Byte Pair Encoding (BPE)（字节对编码）

**字节对编码（Byte Pair Encoding, BPE）**是一种介于字符级和词级之间的编码方法。BPE 会从最初的字符级开始，然后逐步合并频繁出现的字符对（或者子词单元），最终得到更大的词汇单元。这种方法既能处理词级信息，也能处理字符级信息，尤其是在处理词汇外词语时特别有效。

**示例：**

对于句子 "**dog is cute**"，BPE 可能会从字符级开始，然后逐步合并成：

`['d', 'o', 'g', 'is', 'c', 'ute']`

BPE 会根据频率合并高频的字符对或子词单元，因此“is”可能会直接被处理为一个单独的词，而“cute”则可能被分成“c”和“ute”。

**优点**：

- **解决OOV问题**：BPE 可以生成子词单元，因此即使遇到词汇表中没有的词，模型仍可以处理它们。
- **灵活性高**：能够根据频率调整输入单元的粒度，平衡字符级和词级的优缺点。
- **压缩效率高**：BPE 可以显著减少模型处理的序列长度，同时保留词汇的灵活性。

**缺点**：

- **词汇表管理复杂**：词汇表的大小和子词合并的策略需要仔细调整。
- **合并策略非最优**：BPE 使用贪婪的频率合并策略，可能会导致次优合并结果，如同一个词的不同变体被视为不同的输入单元，浪费了词汇表的容量。

### 4. **作者改进的 BPE**

作者对 BPE 的合并策略进行了改进，特别是在处理常见词的变体和跨字符类别合并时做了优化。

#### 改进的 BPE 特点：

- **防止跨字符类别合并**：作者的改进 BPE 方法阻止 BPE 在不同类别的字符（如字母和标点符号）之间进行合并。这避免了将同一个词的多个变体（如 "dog", "dog.", "dog!"）独立处理的问题，从而节省了词汇表空间。
- **允许空格例外**：改进版的 BPE 允许空格与前后字符合并，这可以提高压缩效率，减少不必要的分词碎片化。

#### **优点**：

- **减少不必要的词汇浪费**：防止常见词变体（如 "dog." 和 "dog!"）被独立处理，使词汇表空间得到更有效的利用。
- **更好的压缩率**：通过允许空格合并，能够有效减少子词碎片，提升模型效率。
- **通用性更强**：作者的改进 BPE 可以应用于任何Unicode字符串，不受预定义词汇表限制，并保留了字节级表示的灵活性。

#### **缺点**：

- **仍然有复杂度**：尽管优化了BPE的合并策略，但词汇表的管理和合并规则依然需要复杂的调优。

![GPT2-input_representation](C:\Users\JiangLei\Learn\github\read-notes\assets\GPT2-input_representation.png)
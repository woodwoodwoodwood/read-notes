# DeepSpeed

> Aminabadi等, 《DeepSpeed Inference》.

## 一、问题背景

Transformer模型在推理过程中的性能瓶颈：

- **延迟**：推理任务尤其在在线场景中需要严格的低延迟，这要求优化模型在小批次下的推理效率。
- **吞吐量**：尽管推理要求低延迟，但在生产环境中，还需要同时满足吞吐量需求。
- **资源限制**：许多数据科学家没有访问大量GPU资源的条件，特别是对于拥有数百亿参数甚至上万亿参数的模型。

#### **延迟挑战 (Latency Challenges)**:

在生产环境中的在线应用要求非常严格的延迟性能，尤其是在涉及小批量推理的场景中。这是因为当批量较小时，推理的瓶颈是**将所有模型参数从内存加载到计算单元（寄存器）中所需的时间**。为了满足低延迟要求，关键在于**最大化内存带宽**的利用。

- **小批次的限制**：推理的主要计算瓶颈在于全连接层（fully-connected layers）或线性层，这些层包含了大多数模型权重。因此，必须尽可能接近内存的峰值带宽来加载这些权重，并减少如`layernorm`和`softmax`等操作的内核调度和数据传输开销。
- **多设备并行的必要性**：对于大模型而言，单个设备的内存带宽可能不足以满足推理的延迟要求，这就需要**多设备的聚合内存带宽**，并且还需要优化并行策略以减少设备间的通信开销。
- **并行策略的多样性**：由于Transformer模型架构的多样性，稠密模型和稀疏模型（如MoE模型）需要不同的并行策略，例如稠密模型可以使用**张量并行（tensor parallelism）**和**流水线并行（pipeline parallelism）**，而稀疏模型则需要**专家并行（expert parallelism）**。

> 1. **张量并行（Tensor Parallelism）**
>
> 张量并行是一种将**单层模型中的计算**拆分到多个设备上执行的并行方式。它通过**将单个张量的维度进行切分**，从而分布到多个GPU上进行计算。对于大型的神经网络层来说（比如Transformer中的注意力机制或前馈层），权重矩阵可能非常大。张量并行的基本思路是将这些大型矩阵在多个GPU上拆分，然后各个GPU分别计算拆分后的部分张量，最后再通过通信（如All-Reduce）来整合结果。
>
> - **示例**：假设某一层的权重矩阵为$W$，我们可以把$W$按列切分，分配给两个GPU来分别计算矩阵乘法操作。每个GPU只处理部分列，而不是整个矩阵。计算完成后，各GPU将结果合并。
>
> **优点**：有效减少了每个GPU上的计算量和内存需求，适合大模型的层。
>
> **挑战**：各GPU之间需要频繁通信来交换中间结果，通信开销较大。
>
> 论文：Megatron-LM
>
> 2. **数据并行（Data Parallelism）**
>
> 数据并行是最常见的一种并行训练方式，它的基本原理是**每个GPU拥有完整的模型副本，但每个GPU处理不同的数据子集**。具体而言，每个GPU都会在自己的子集上进行正向传播和反向传播，计算出梯度后，所有GPU会将它们的梯度同步（通常通过All-Reduce操作），并且使用同步后的全局梯度来更新每个GPU上模型的参数。
>
> - **示例**：假设我们有四个GPU和一个包含1000个样本的数据集，每个GPU可以处理250个样本。每个GPU在独立的数据子集上计算损失和梯度，之后这些梯度会被汇总起来，再用于更新模型。
>
> **优点**：实现简单，并且扩展性好，能够轻松在多GPU上并行训练。
>
> **挑战**：随着模型变大，每个GPU上需要存储的完整模型副本可能会超出单个GPU的内存限制。另外，在每一轮训练结束时，所有GPU之间需要频繁的梯度同步操作，通信开销较大。
>
> 3. **流水线并行（Pipeline Parallelism）**
>
> 流水线并行是一种适合**深层模型**的并行方式。在流水线并行中，模型的**不同层**分配到不同的设备上，以便在多个设备上同时进行正向传播或反向传播。流水线并行的核心思想是将模型分成若干个“阶段”，每个阶段在不同的设备上执行，数据通过这些阶段流动，类似于工厂流水线。
>
> - **示例**：假设我们有一个深度神经网络包含6层，并且有3个GPU。我们可以将前两层放到GPU1，中间两层放到GPU2，最后两层放到GPU3。当第一批数据在GPU1上计算第一层和第二层时，第二批数据可以在GPU2上同时计算第三层和第四层，以此类推。
>
> **优点**：适用于深度模型，可以充分利用多GPU的计算资源，并减少每个GPU上的内存需求。
>
> **挑战**：因为流水线需要等待数据从一个阶段传递到下一个阶段，所以可能会引入“流水线气泡”，导致并行效率下降。
>
> 论文：GPipe，PipeDream
>
> 4. **模型并行（Model Parallelism）**
>
> 模型并行指的是**将模型不同部分的计算分配到不同的GPU上**。在模型并行中，不同的设备处理模型的不同部分，通常用于非常大的模型，在单个GPU上无法存储整个模型的情况下使用。模型并行可以看作是张量并行和流水线并行的结合，因为它可以涉及不同层（流水线）或同一层的不同部分（张量）的分配。
>
> - **示例**：一个非常大的Transformer模型，如果一个GPU无法容纳所有参数，可以将不同的Transformer层分配到不同的GPU上。第一个GPU处理第1层和第2层，第二个GPU处理第3层和第4层，依此类推。每个GPU处理完成自己负责的层后，将输出结果传递给下一个GPU。
>
> **优点**：适用于无法在单个设备上完全存储的大模型。
>
> **挑战**：不同部分的模型之间的依赖关系可能导致GPU间频繁通信，并且难以负载均衡。如果分配不当，可能导致部分GPU闲置等待其他GPU完成计算。

#### **吞吐量挑战 (Throughput Challenges)**:

1. **推理内核（Inference Kernels）**

- **高内存带宽利用率**：推理内核在小批量数据的情况下，必须优化内存带宽的使用。这是因为推理通常用于实时或低延迟应用（如在线服务），其输入数据量通常较小，导致计算密集型操作（如矩阵乘法）对内存带宽的依赖增加。推理过程的性能受制于从内存中快速加载模型参数（权重）到计算单元（如GPU寄存器）中的速度。
- **高计算利用率**：推理内核还需要尽可能地提高计算利用率，确保在执行模型计算时，计算单元的负载能够得到充分利用。由于小批量数据的限制，推理需要在内存读取和计算之间实现良好的重叠，以降低延迟。
- **挑战**：由于推理通常在小批量条件下运行，内存的读取时间相对较长，因此开发者必须设计内核以最大限度地提高内存带宽利用率，同时保持高计算效率。这要求在优化算法上进行更多的考量，特别是在内存访问模式、数据布局和计算重叠方面。

2. **训练内核（Training Kernels）**

- **高计算利用率**：训练内核的主要目标是在较大的批量大小下实现高计算利用率。由于训练过程涉及大量的计算，通常可以通过增加批量大小来充分利用计算资源。
- **大批量的优势**：在较大的批量大小下，计算单元的并行能力可以得到更充分的发挥，导致整体计算效率提高。训练的目标通常是优化损失函数，而在大批量的情况下，计算的重叠程度会提高，内存带宽利用率可能不会是性能瓶颈。

![image-20241018162632033](./assets/image-20241018162632033.png)

除了低延迟之外，生产中的推理工作还需要达到特定的吞吐量目标来满足成本预算。

- **小批量推理**：在小批次下，推理任务仍然受内存带宽的限制。如果计算能够与权重读取同时进行，推理延迟不会显著增加。因此，**最大化模型权重读取和计算的重叠**是提高吞吐量的关键。
- **大批量推理**：对于大批次的推理任务，推理计算的依赖性不同于训练。例如，生成式Transformer模型需要处理**生成的每个token与下一个token之间的依赖关系**，这在训练过程中并不存在。这种依赖性增加了推理的内存需求，特别是对于需要使用流水线并行以适应内存容量的大模型，这种依赖还需要重新安排流水线调度以确保所有设备都保持忙碌状态。

#### **资源受限情况下的可行性挑战 (Feasibility Challenges under Limited Resources)**:

现代的Transformer模型，特别是那些拥有数百亿到数万亿参数的模型，内存需求远远超出单个GPU甚至单个节点的GPU聚合内存容量。举例来说，MT-NLG 530B模型推理时需要1TB的GPU内存，这需要超过24块NVIDIA A100 GPU，而这种大规模GPU资源并不是所有数据科学家都可以轻松获得的。

***

## 二、DeepSpeed Inference的解决方案

#### 1) DeepSpeed Transformer

- **GPU专用解决方案**：DeepSpeed Transformer是一个专门设计的GPU解决方案，旨在最小化延迟并最大化吞吐量，支持稠密和稀疏的Transformer模型。
- **三层系统架构**：
  1. **单GPU内核**：为低批量大小优化内存带宽利用率，同时在大批量大小时实现高吞吐量。
  2. **多GPU稠密Transformer层**：通过张量切片和为推理优化的流水线并行，支持稠密Transformer模型在多个GPU上的扩展。
  3. **大规模稀疏Transformer层**：结合多种并行技术和通信优化策略，支持数百GPU的MoE（Mixture of Experts）Transformer层推理，同时最小化单GPU稀疏计算的开销。
- **综合系统**：通过这种分层的方法，每一层解决延迟挑战的不同方面，如批量大小、稠密模型扩展和稀疏模型扩展，确保各层之间的兼容性，并建立在彼此之上。

#### 2) ZeRO-Inference

- **异构解决方案**：ZeRO-Inference是一个基于GPU、CPU和NVMe的异构解决方案，旨在通过最小化GPU资源来解决内存挑战。
- **资源受限的推理**：与DeepSpeed Transformer不同，ZeRO-Inference适用于对延迟敏感性要求较低但资源有限的应用。它允许在单个或多个GPU上推理数百亿参数的模型，只要有足够的CPU或NVMe内存来存储模型参数。
  - **更高的每GPU效率**：即使模型能够适应聚合GPU内存，ZeRO-Inference通过支持更大的批量大小来提供比DeepSpeed Transformer更好的每GPU效率。	

#### A. 不同批量大小下的推理挑战（Inference Challenges on Different Batch Sizes）

##### 1. 小批量推理的性能限制

- **内存带宽利用率**：在小批量推理中，性能受到内存带宽利用率的限制。由于每次仅处理少量数据，内存读取和写入操作的频率相对较高，这导致内存带宽没有得到充分利用。
- **主要挑战**：
  - **内核调用开销**：在小批量情况下，执行Transformer层的不同操作需要频繁地调用内核。这种频繁的调用导致了内核调用开销增大，从而影响了推理性能。
  - **数据传输开销**：每次内核调用会将计算结果写入全局内存，而在下一个内核调用时，GPU核心又需要从全局内存读取这些数据。这种GPU核心与全局内存之间的数据传输增加了额外的开销，影响了整体效率。
  - **库的适应性不足**：目前的cuBLAS和CUTLASS GeMM库在极小批量大小下没有进行良好的优化，无法有效利用内存带宽。因此，即使有小批量的计算需求，现有库的性能也无法满足推理的要求。

##### 2. 大批量推理的性能限制

- **计算利用率**：与小批量推理不同，大批量推理的性能通常受计算利用率的限制。对于像GeMM这样的计算密集型操作，在Transformer层内部可以使用cuBLAS和CUTLASS库实现较高的计算利用率。
- **主要挑战**：
  - **内核启动开销**：尽管计算密集型操作的利用率较高，但在多个内核（除了GeMM以外的其他内核）之间的数据传输和内核启动开销会限制整体的利用率。这意味着在大批量情况下，即使GeMM本身效率很高，整体的性能仍可能受到影响。

##### 3. 优化策略

为了应对上述挑战，论文提出了两种优化技术：

###### i) Deep-Fusion

传统的算子融合中，主要针对**元素级**操作，而Transformer中的许多操作（如矩阵乘法、归约等）由于数据依赖关系难以融合。这会导致在GPU上性能不佳，因为这些操作需要额外的同步和内核启动。

Deep-Fusion的核心思想是通过更高级别的抽象和优化策略，将多个算子合并为一个更复杂的操作，以减少内存访问和内核启动的开销。其主要特点包括：

- **跨算子融合**：不仅限于元素级操作，而是将多个不同类型的算子结合在一起。
- **数据流优化**：利用数据流图，优化数据依赖关系，减少全局内存同步的需求。
- **动态调度**：根据实时数据和计算需求，动态调整操作顺序和执行策略，以提高资源利用率。

**优势**：

- **减少内核调用次数**：通过融合多个内核，可以显著降低内核调用的频率，从而减小由频繁调用引起的性能损失。
- **降低数据传输开销**：在内核融合的过程中，计算结果可以在不同的计算步骤之间共享，减少了需要在全局内存中进行多次读写的数据传输。

Deep-Fusion的实现一般包括以下几个步骤：

1. **算子分析**：识别出模型中可以融合的算子，分析它们之间的依赖关系。
2. **图优化**：构建计算图，对图进行优化，尝试减少不必要的内存访问和同步。
3. **融合执行**：将多个操作组合为一个单一的内核调用，以便在GPU上高效执行。



###### ii) SBI-GeMM: Custom GeMM for Small Batch Size

使用Transformer等模型时，批量大小（batch size）会影响模型的性能和效率。小批量大小的推理通常会导致计算资源的浪费，因为矩阵乘法（GeMM）操作的并行化效果不佳，且会增加内核启动和内存访问的开销。

- **设计目的**：自定义GeMM内核的目的是在较小批量大小时提高内存带宽的利用率，同时也允许使用Deep-Fusion进行融合。
- **内存带宽优化**：这种定制内核通过特定的算法和数据布局来优化内存的读取和写入模式，确保即使在小批量的情况下，内存带宽也能得到良好的利用，从而提高推理性能。

##### C. SBI-GeMM：针对小批量大小的自定义GeMM

###### 1. 切块策略（Tiling Strategies）

- **概念**：切块策略是一种将计算分成较小部分（或“块”）的方式，以便更有效地利用内存和计算资源。在GeMM中，切块允许操作在较小的输出维度上执行，从而提高并行性和内存带宽的利用。
- **实现方法**：
  - **狭窄矩阵乘法调度**：图1(a)展示了狭窄矩阵乘法的GeMM调度。该方法首先沿输出维度进行切块。通过在单个内核中实施GeMM，将归约限制在一个切块内。
  - **小模型处理**：对于小模型，当输出维度不足以创建足够的并行切块以实现良好的内存带宽时，也会对输入维度进行切块。这意味着对于小批量情况，GeMM可能会实现为两个内核，从而允许跨切块进行归约。

###### 2. 协作组归约（Cooperative-Group Reduction）

- **问题**：使用上述切块策略后，线程块中的每个warp负责生成输出切块的部分归约结果，但通常情况下，归约操作通过共享内存中的二叉树实现，这需要多次warp级别的同步，导致性能瓶颈。
- **优化方法**：
  - **共享内存中的数据布局转置**：为了避免性能瓶颈，研究团队在共享内存中执行单次数据布局转置，使得相同输出元素的部分结果在内存中是连续的。这意味着可以通过一个warp使用协作组集体操作直接在寄存器中进行归约，而无需额外的同步。
  - **写入共享内存**：最终，每个warp的第一个线程保存最终结果并将其写入共享内存。由于结果在共享内存中是连续的，能够实现对全局内存的合并写入，提高内存带宽利用率。

###### 3. 利用完整缓存行（Leveraging Full Cache-line）

- **GPU架构的挑战**：在GPU架构中，L1缓存行的大小为128字节，而在warp中每个线程的共轭内存访问通常只能读取单个FP16或INT8元素，无法充分利用完整的缓存行。这样会导致内存访问效率低下。
- **解决方案**：
  - **在初始化时转置权重矩阵**：通过在初始化时转置权重矩阵，使每列的M行在内存中连续，从而允许每个线程沿输入维度读取M个元素。图1(b)展示了这种转置的效果。
  - **M的选择**：根据128字节的缓存行，对于半精度（FP16）设置M为2，对于INT8数据类型设置M为4。这种设计确保了内存访问时能够充分利用缓存行，提高整体性能。

![image-20241018173223068](./assets/image-20241018173223068.png)

![image-20241018173733922](./assets/image-20241018173733922.png)

1. **输入和输出**：
   - **输入**：图的顶部部分显示了输入数据的结构，可能是一个矩阵或张量。
   - **输出**：在图的右侧，输出部分表示经过GeMM操作后的结果，将输入通过矩阵乘法处理后生成的输出。
2. **批量（Batch）**：
   - 输入数据首先以批量形式传入，这意味着多个样本同时被处理。在图中，这部分通过多个输入块表示。
3. **局部权重读取（Local Weight Read）**：
   - 在每个计算块（Block）内，局部权重（例如矩阵的部分数据）被读取并用于后续计算。这减少了对全局内存的频繁访问，提高了内存带宽的利用率。
4. **块（Blocks）**：
   - 图中有多个“Block”，每个Block执行其自身的局部GeMM运算。每个Block内的线程将共同工作，以利用共享内存和计算资源。
5. **局部GeMM（Local GeMM）**：
   - 每个Warp（图中标注的“Warps”）内都有多个局部GeMM实例。这些局部GeMM操作在共享内存中执行，允许线程之间更快的数据传递和计算。
6. **共享内存（Shared Memory）**：
   - 共享内存用于存储中间结果，特别是在执行归约操作时。共享内存的使用可以减少对全局内存的需求，从而提高速度。
7. **转置（Transpose）**：
   - 在局部GeMM计算后，数据会在共享内存中进行转置。这是为了将部分结果整理为连续的存储布局，使得后续的归约操作更高效。
8. **归约（Reduce）**：
   - 归约过程将局部结果整合为最终输出。通过在共享内存中进行归约，可以有效减少线程之间的同步需求。
9. **线程（Threads）和Warps**：
   - 每个Warp由32个线程组成。在图中，显示了不同Warp的线程，并且每个线程在其Warp中都有一个索引（标记为“Lanes”）。通过将多个线程组合在一起执行计算，能够更好地利用GPU的并行能力。

![image-20241101172726612](./assets/image-20241101172726612.png)
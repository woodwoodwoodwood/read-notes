{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highway 网络\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, num_layers=2):\n",
    "        super(Highway, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.linears = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.gates = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear, gate in zip(self.linears, self.gates):\n",
    "            gate_output = torch.sigmoid(gate(x))\n",
    "            nonlinear_output = F.relu(linear(x))\n",
    "            x = gate_output * nonlinear_output + (1 - gate_output) * x  # Gated connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型架构\n",
    "class BiLSTMWithResidual(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=4096, proj_dim=512, num_layers=2, vocab_size=100, char_embed_dim=64):\n",
    "        super(BiLSTMWithResidual, self).__init__()\n",
    "\n",
    "        # 字符嵌入\n",
    "        self.char_embedding = nn.Embedding(vocab_size, char_embed_dim)\n",
    "        \n",
    "        # 卷积层用于字符 n-gram\n",
    "        self.char_conv = nn.Conv1d(in_channels=char_embed_dim, out_channels=2048, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 两层 Highway 网络\n",
    "        self.highway = Highway(size=2048)\n",
    "        \n",
    "        # 线性投影到 512 维\n",
    "        self.proj = nn.Linear(2048, proj_dim)\n",
    "        \n",
    "        # 双向 LSTM\n",
    "        self.lstm = nn.LSTM(input_size=proj_dim, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                            bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # 线性投影将 LSTM 输出从 4096 维降维到 512 维\n",
    "        self.proj_lstm = nn.Linear(2 * hidden_dim, proj_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 假设输入 x 是字符的索引\n",
    "        # 字符嵌入\n",
    "        char_embeds = self.char_embedding(x)  # (batch_size, seq_len, char_embed_dim)\n",
    "        \n",
    "        # 转换为卷积输入格式: (batch_size, char_embed_dim, seq_len)\n",
    "        char_embeds = char_embeds.permute(0, 2, 1)\n",
    "        \n",
    "        # 卷积操作\n",
    "        conv_output = self.char_conv(char_embeds)  # (batch_size, 2048, seq_len)\n",
    "        conv_output = F.relu(conv_output)\n",
    "        \n",
    "        # 转置回原来的形状: (batch_size, seq_len, 2048)\n",
    "        conv_output = conv_output.permute(0, 2, 1)\n",
    "        \n",
    "        # 通过 Highway 网络\n",
    "        highway_output = self.highway(conv_output)  # (batch_size, seq_len, 2048)\n",
    "        \n",
    "        # 线性投影到 512 维\n",
    "        proj_output = self.proj(highway_output)  # (batch_size, seq_len, 512)\n",
    "        \n",
    "        # 通过双向 LSTM\n",
    "        lstm_output, _ = self.lstm(proj_output)  # (batch_size, seq_len, 2 * hidden_dim)\n",
    "        \n",
    "        # 添加残差连接（从输入到输出的连接）\n",
    "        residual_output = proj_output + lstm_output  # (batch_size, seq_len, 2 * hidden_dim)\n",
    "        \n",
    "        # 最终投影到 512 维\n",
    "        final_output = self.proj_lstm(residual_output)  # (batch_size, seq_len, 512)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "vocab_size = 100  # 假设字符表大小为100\n",
    "input_dim = 64  # 假设字符嵌入维度为64\n",
    "\n",
    "# 随机生成字符索引输入\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# 初始化模型\n",
    "model = BiLSTMWithResidual(input_dim=input_dim, vocab_size=vocab_size)\n",
    "\n",
    "# 前向传播\n",
    "output = model(x)\n",
    "print(output.shape)  # 输出维度应该是 (batch_size, seq_len, 512)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
